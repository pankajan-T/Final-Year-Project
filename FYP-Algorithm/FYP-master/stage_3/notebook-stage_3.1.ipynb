{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stage 3 - Deep Reinforcement Learning for Implementing Infinite Impulse Response (IIR) Filters for Complex Interference Situations - part 1**\n",
    "\n",
    "## Scope - *developing lower order (2nd and 4th) IIR filters for scaled and multicarrier interference*\n",
    "References:- Chapter 11 of *Digital Signal Processing: signals, systems, and filters* by *Andreas Antoniou*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous stage, stage 2, we discussed about implementing 2nd and 4th order IIR filters solely for ***constant single-carrier non-overlapping interference situations***. \n",
    "- In this stage, we try to analyze the capability of the current DDPG model and the filter-emulating environment to learn lower order (and potentially higher order) IIR filters for dfferent interference situations, including scale variations, multicarrier components, and overlapping interferences with AWGN noise. \n",
    "\n",
    "\n",
    "- One thing to note here is that, although the current model (as by 31/01/2024) for 4th order IIR is learning *sufficiently good* filters, giving average SNR values higher than 30dB, 50% of the time, for the simple interference situations specified above, those models are still not capable of achieving optimal filters. (Althoug it is impossible to find an optimal filter for a given interference situation, there are surely better filters than what we learn.) And developing DRL models that are capable of learning such optimal filters is still a question for research under this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, copy, json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../stage_2/envs/')\n",
    "\n",
    "# import the DDPG model\n",
    "from DDPGwithCustomNetDepths import DDPGAgentwithCustomNetworkDepths\n",
    "\n",
    "# import the environment\n",
    "from ReceiverEnvWithArbitaryOrderIIRwithDelayedTargetSNR import ReceiverEnvWithArbitaryOrderIIRwithDelayedTargetSNR\n",
    "\n",
    "# train and test functions\n",
    "from stage2_helper import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common constants\n",
    "S = 100\n",
    "SAMPLING_FREQ = 44_100 # Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Single-Carrier Interfernce**\n",
    "\n",
    "- In the previous stage, we trained a model for constant single-carrier non-overlapping interference. Now, we shall see the behavior of learning for the same type of interference *while changing the power of the interference using `interference_scalar` variable*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for single carrier interfernce\n",
    "CUT_OFF_FREQ = 5_000 # Hz\n",
    "INTERFERENCE_CENTER_FREQ = 15_000 # Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## START TESTING INTERFERENCE SCALAR 0.25 ########################################\n",
      "creating action space with 9 dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 16:05:10.507364: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-07 16:05:10.514871: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------episode no: 1--------------------------------------------------\n",
      "audio name: 'arms_around_you-MONO'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "the specified audio file doesn't exist: given ../stage_1/audio_files/arms_around_you-MONO.wav",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 65\u001b[0m\n\u001b[1;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m DDPGAgentwithCustomNetworkDepths(\n\u001b[1;32m     50\u001b[0m     input_dims  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m     51\u001b[0m     n_actions   \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     action_activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m reward_history, action_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAUDIO_NUM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNO_OF_STEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m train_reward_history_subarr\u001b[38;5;241m.\u001b[39mappend(reward_history)\n\u001b[1;32m     67\u001b[0m train_action_history_subarr\u001b[38;5;241m.\u001b[39mappend(action_history)\n",
      "File \u001b[0;32m/mnt/h/Final_Year_project/FYP-Algorithm/FYP-master/stage_3/../stage2_helper.py:154\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_, env_, audio_num, max_num_steps, noise_schedule, noise_step_size, noise_exponent, reward_history, action_history)\u001b[0m\n\u001b[1;32m    151\u001b[0m step_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# reset the environment\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreset_all\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_num\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43maudio_num\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# FOR SINGLE EPISODE\u001b[39;00m\n\u001b[1;32m    155\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# feed the state to the agent (model) and get an action\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/h/Final_Year_project/FYP-Algorithm/FYP-master/stage_3/../stage_2/envs/ReceiverEnvWithArbitaryOrderIIRwithDelayedTargetSNR.py:137\u001b[0m, in \u001b[0;36mReceiverEnvWithArbitaryOrderIIRwithDelayedTargetSNR.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    133\u001b[0m     audio_num \u001b[38;5;241m=\u001b[39m options[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_num\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# i = np.random.randint(low=1, high=self.audio_num) # len(train_audio_names)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# create the target and jammed signals\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m target_signal, jammed_signal \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_target_and_jammed_signals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_audio_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43maudio_num\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcut_off_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterference_center_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterference_center_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterference_scalar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterference_scalar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43msignal_partition_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_signal \u001b[38;5;241m=\u001b[39m target_signal\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjammed_signal \u001b[38;5;241m=\u001b[39m jammed_signal\n",
      "File \u001b[0;32m/mnt/h/Final_Year_project/FYP-Algorithm/FYP-master/stage_3/../stage2_helper.py:58\u001b[0m, in \u001b[0;36mcreate_target_and_jammed_signals\u001b[0;34m(audio_name, truncation_freq, interference_center_freq, signal_partition_size, interference_scalar, audio_file_dir, save_files)\u001b[0m\n\u001b[1;32m     56\u001b[0m audio_src_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(audio_file_dir, audio_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audio_src_file):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe specified audio file doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist: given \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_src_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m sampling_rate, audio \u001b[38;5;241m=\u001b[39m wavfile\u001b[38;5;241m.\u001b[39mread(audio_src_file)\n\u001b[1;32m     60\u001b[0m sampling_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39msampling_rate\n",
      "\u001b[0;31mException\u001b[0m: the specified audio file doesn't exist: given ../stage_1/audio_files/arms_around_you-MONO.wav"
     ]
    }
   ],
   "source": [
    "# define the filter order\n",
    "ORDER = 4\n",
    "\n",
    "# train parameters\n",
    "AUDIO_NUM = 1\n",
    "NO_OF_TESTS = 4\n",
    "NO_OF_STEPS = 4_000\n",
    "\n",
    "# define the interference scalars to check\n",
    "interference_scalars = [0.25, 0.5] # , 1.25, 1.5, 1.75, 2\n",
    "\n",
    "train_reward_history_arr = [] # 3D array\n",
    "train_action_history_arr = [] # 3D array\n",
    "test_avg_reward_arr      = [] # 2D array\n",
    "test_avg_action_arr      = [] # 2D array\n",
    "model_arr = [] #2D array\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(interference_scalars)):\n",
    "\n",
    "    print(\"#\"*40 + f\" START TESTING INTERFERENCE SCALAR {interference_scalars[i]} \" + \"#\"*40)\n",
    "\n",
    "    # define the environment\n",
    "    env = ReceiverEnvWithArbitaryOrderIIRwithDelayedTargetSNR(\n",
    "        order = ORDER,\n",
    "        S = S,\n",
    "        cut_off_freq = CUT_OFF_FREQ,\n",
    "        interference_center_freq = INTERFERENCE_CENTER_FREQ,\n",
    "        interference_scalar = interference_scalars[i],\n",
    "        zero_magnitude_mapping = None,\n",
    "        gradient = None,\n",
    "        fix_zeros_magnitude = False,\n",
    "        automatic_gain = False,\n",
    "        SNR_as_dB = True,\n",
    "        show_effect = False,\n",
    "    )\n",
    "\n",
    "    # initialize the sub-arrays\n",
    "    train_reward_history_subarr = [] # 2D array\n",
    "    train_action_history_subarr = [] # 2D array\n",
    "    test_avg_reward_subarr      = [] # 1D array\n",
    "    test_avg_action_subarr      = [] # 1D array\n",
    "    model_subarr = [] # 1D array\n",
    "\n",
    "    for j in range(NO_OF_TESTS):\n",
    "\n",
    "        # initialize a DDPG model\n",
    "        model = DDPGAgentwithCustomNetworkDepths(\n",
    "            input_dims  = env.observation_space.shape,\n",
    "            n_actions   = env.action_space.shape[0],\n",
    "            alpha       = 0.0001, # learning rate of actor\n",
    "            beta        = 0.001,  # learning rate of critic\n",
    "            gamma       = 0,      # ***** decreasing the discounting factor *****\n",
    "            tau         = 0.001,\n",
    "            critic_dims = [[256], [512, 256], []],\n",
    "            actor_dims  = [256, 128],\n",
    "            batch_size  = 256,\n",
    "            buffer_size = 4_000,\n",
    "            noise       = 0.01,\n",
    "            action_activation = 'sigmoid'\n",
    "        )\n",
    "\n",
    "        # train the model\n",
    "        reward_history, action_history = train(model, env, audio_num=AUDIO_NUM, max_num_steps=NO_OF_STEPS)\n",
    "        train_reward_history_subarr.append(reward_history)\n",
    "        train_action_history_subarr.append(action_history)\n",
    "\n",
    "        # test the trained model\n",
    "        test_rewards, test_actions = test(model, env, audio_num=AUDIO_NUM, num_steps=NO_OF_STEPS, fixed_action=None)\n",
    "        test_avg_reward_subarr.append(mean_reward := np.mean(test_rewards))\n",
    "        test_avg_action_subarr.append(np.average(np.array(test_actions), axis=0))\n",
    "        print(f\"average test performance: {mean_reward}dB\")\n",
    "\n",
    "        # save the model\n",
    "        model_subarr.append(model)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        print(\"=\"*30 + f\" execution time: {round((end - start), 3)}s \" + \"=\"*30)\n",
    "\n",
    "    train_reward_history_arr.append(train_reward_history_subarr)\n",
    "    train_action_history_arr.append(train_action_history_subarr)\n",
    "    test_avg_reward_arr.append(test_avg_reward_subarr)\n",
    "    test_avg_action_arr.append(test_avg_action_subarr)\n",
    "\n",
    "    model_arr.append(model_subarr)\n",
    "    \n",
    "    print(\"#\"*41 + f\" TESTING INTERFERENCE SCALAR {interference_scalars[i]} IS OVER \" + \"#\"*41)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------- saving the data -----------------------------------------------\n",
    "test_no = 1\n",
    "dir_path = 'logs/notebook-stage_3.1/'\n",
    "folder_name = f\"test_{test_no}\"\n",
    "folder_path = os.path.join(dir_path, folder_name)\n",
    "os.makedirs(folder_path)\n",
    "\n",
    "# saving the train reward history\n",
    "file_name = f\"interference_scaling-scales_{0.25}_{0.5}-train_rewards.npy\"\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "np.save(file_path, train_reward_history_arr)\n",
    "\n",
    "# saving the train action history\n",
    "file_name = f\"interference_scaling-scales_{0.25}_{0.5}-train_actions.npy\"\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "np.save(file_path, train_action_history_arr)\n",
    "\n",
    "# saving the test average rewards\n",
    "file_name = f\"interference_scaling-scales_{0.25}_{0.5}-test_avg_rewards.npy\"\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "np.save(file_path, test_avg_reward_arr)\n",
    "\n",
    "# saving the test average rewards\n",
    "file_name = f\"interference_scaling-scales_{0.25}_{0.5}-test_avg_actions.npy\"\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "np.save(file_path, test_avg_action_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_subarr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m model_\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model_.critic.summary()\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model_ = model_subarr[0]\n",
    "model_.actor.summary()\n",
    "# model_.critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
